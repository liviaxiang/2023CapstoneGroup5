---
title: "Customer Success Analysis for Swire Coca-Cola"
author: "Li Xiang"

output:
  html_document: default
  pdf_document: default
---
## 1. Business Problem Statement 
Swire Coca-Cola is seeking a solution to predicting the profitability of new partnerships with local businesses.

#### Benefits of Solution
By identifying and partnering with local businesses that align with Swire Coca-Cola and its target audience, they can increase sales and revenue for their product.<br>
This will enable them to make more informed decisions, increase their reach in specific local markets, and provide a benefit for the local business.<br>
Additionally, we will use data visualization tools to present the insights in an understandable format to the stakeholders. This will help them make data-driven decisions that align with the company's strategy.<br>

#### Success Metrics
Success metrics for the model of predicting new partnerships with local businesses for Swirl Coca-Cola products may include:<br>
1. Revenue increase: The increase in revenue generated by the new partnerships compared to previous years or to a control group.<br>
2. ROI: The return on investment of the data science project, measured by the increase in revenue generated by the new partnerships compared to the projectâ€™s cost.<br>
3. Accuracy of predictions: The accuracy of the model's predictions in identifying potential partners and predicting the likelihood of successful partnerships.<br>

#### Approach
By utilizing predictive models such as linear regression, logistic regression, and other possible models to analyze historical data and market trends to identify local businesses that align with the brand and target audience and predict the likelihood of successful partnerships.

#### Scope and deliverables
The deliverables for this project will be a visual presentation and a written report summarizing our exploratory data analysis, model selection, evaluation and deployment process, and recommendations for applying our results. <br> 
The presentation slides, written report, and all project code files will be provided to Swire Coca-Cola in the form of a GitHub repository accompanied by documentation explaining the repository contents, access, and organization.

#### Details
This project will be completed by student team members Katelyn Candee, Li Xiang and Vicky Mao by April 13, with progress checkpoints overseen by University of Utah faculty advisor Jeremy Morris on or before the following dates:<br>
1. Exploratory data analysis - February 19 <br>
2. Model selection, evaluation and deployment - March 19<br>
3. Practice project presentation - April 9<br>

Project team members may be reach at:<br>
1. Katelyn Candee - (203) 823-3129 - u1398566@utah.com <br>
2. Li Xiang - (385) 335-4332 - u1328517@utah.edu<br>
3. Vicky Mao - (801) 970-0482 - u1132288@utah.edu<br>


## 2. Data Analysis
```{r}
# Read in the data
#A factor with levels 0 and 1 indicating whether the customer purchased Citrus Hill (1) or Minute Maid Orange Juice (0).

#object_data<-read.csv("FSOP Combine Data.csv")
object_data<-read.csv("FSOP Combine Data Utah.csv")
```
### 2.1 Clean the data
There is a customer table and sales table provided by the Swire Coca Cola, I joined the two tables by using customer id. 
The joined table is too large for my computer or R-Studio cloud to run the models. 
Therefore I filtered the data, which now only contains data for Utah state.

Change some discrete attributes into factor type.
```{r}
#clean the data, mutate the factor attributes 
object_data[3:6] <- lapply(object_data[3:6], as.factor)
#object_data[1:2] <- lapply(object_data[1:5], as.numeric)

object_data$SALES_OFFICE_DESCRIPTION <- as.factor(object_data$SALES_OFFICE_DESCRIPTION)
object_data$DELIVERY_PLANT_DESCRIPTION <- as.factor(object_data$DELIVERY_PLANT_DESCRIPTION)
object_data$ADDRESS_CITY <- as.factor(object_data$ADDRESS_CITY)
object_data$ZIP_CODE <- as.factor(object_data$ZIP_CODE)
object_data$COUNTY <- as.factor(object_data$COUNTY)

object_data[24:30] <- lapply(object_data[24:30], as.factor)

object_data$MIN_POSTING_DATE <- as.Date(object_data$MIN_POSTING_DATE)
object_data$MAX_POSTING_DATE <- as.Date(object_data$MAX_POSTING_DATE)
object_data$ON_BOARDING_DATE <- as.Date(object_data$ON_BOARDING_DATE)


summary(object_data)

```

```{r, echo = FALSE}
# Check for missing values
sum(is.na(object_data))
#Remove the na 
#object_data %>%
#  na.omit(object_data)

```

```{r}
# Check for missing values
sum(is.na(object_data))

```
### 2.2 Explore and Plot the relationships
The gross_profit_dead_net would be the predictor variable.

-Explore the correlation of the attributes

```{r}
# Checking for multicollinearity and plotting correlations between predictors
#install.packages("ggcorrplot")
library(psycho) 
library(tidyverse)
library(ggplot2)
correl <- cor(object_data[c(7,8,9,10,11,12)])
ggcorrplot::ggcorrplot(correl,hc.order=TRUE, type="lower",lab=TRUE) 
cor(correl)
```
The graph above shows that some attributes are highly correlated, like INVOICE_PRICE and DEAD_NET, COGS and DEAD_NET, COGS and INVOICE_PRICE.



## 3. Split Data for training and testing
```{r}

set.seed(123)
dt1 <-sample(nrow(object_data), nrow(object_data)*.7)
trainData<-object_data[dt1,]
testData<-object_data[-dt1,]

```


```{r}
predictors <-trainData[,-c(1,2,11)]

pred_res = trainData$GROSS_PROFIT_DEAD_NET

test_predictors <-testData[,-c(1,2,11)]

```



## 4. Compare the models
The predictor is GROSS_PROFIT_DEAD_NET, I am using the models as below to explore.
Multiple Linear Regression
Ridge Regression
Lasso Regression
Elastic Net Regression
Gradient Boosting Regression
Neural Network Regression

### 3.1 Linear Regression Model
```{r}
#build the model to check which predictors have significant influence

#linear_regression<- lm(GROSS_PROFIT_DEAD_NET~., data= trainData)

#lineae_predictions <- predict(linear_regression, newdata = testData)


#postResample(lineae_predictions, testData$GROSS_PROFIT_DEAD_NET)
#mse <- mean((test_response - ridge_predictions)^2)
#print(paste("Test set mean squared error:", mse))

#summary(model1) # will give output for each level of each categorical predictor

```

**Penalized Regression**

```{r}
#Selecting relevant predictors using penalized regression
#LASSO

#define matrix of predictor variables
predictors_matrix <-data.matrix(predictors)

#install.packages("glmnet")
library(glmnet)

#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(predictors_matrix, pred_res, alpha = 1, standardize = TRUE, nfolds = 5)

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda


#produce plot of test MSE by lambda value
plot(cv_model) 


#find coefficients of best model
best_lasso1 <- glmnet(predictors_matrix, pred_res, alpha = 1, lambda = best_lambda, standardize = TRUE)
coef(best_lasso1)


```

```{r}
#install.packages("caret")
library(caret)
test_predictors <- data.matrix(test_predictors)
test_response <- testData$GROSS_PROFIT_DEAD_NET
lasso_predictions <- predict(best_lasso1, newx = test_predictors)


postResample(lasso_predictions, testData$GROSS_PROFIT_DEAD_NET)

mse <- mean((test_response - lasso_predictions)^2)
print(paste("Test set mean squared error:", mse))
```

Strengths:

Reduces overfitting and can improve model performance.
Can be used to identify important predictor variables and their relative contributions to the model.
Can handle highly correlated predictor variables.

Weaknesses:

Assumes a linear relationship between the predictor and outcome variables, which may not always be the case.
Requires tuning of the regularization parameter to avoid underfitting or overfitting.
Can be computationally intensive when there are many predictor variables.





```{r}
#Ridge
#perform k-fold cross-validation to find optimal lambda value
cv_ridge <- cv.glmnet(predictors_matrix, pred_res, alpha = 0,family = 'gaussian', standardize = TRUE, nfolds = 5)

optimal_lambda <- cv_ridge$lambda.min
optimal_lambda

#produce plot of test MSE by lambda value
plot(optimal_lambda) 

#find coefficients of best model
best_ridge1 <- glmnet(predictors_matrix, pred_res, alpha = 0, lambda = optimal_lambda, standardize = TRUE)
coef(best_ridge1)

```
Strengths:

Reduces overfitting and can improve model performance.
Can be used to identify important predictor variables and their relative contributions to the model.
Can handle highly correlated predictor variables.

Weaknesses:

Assumes a linear relationship between the predictor and outcome variables, which may not always be the case.
Requires tuning of the regularization parameter to avoid underfitting or overfitting.
Can be computationally intensive when there are many predictor variables.


```{r}
ridge_predictions <- predict(best_ridge1, newx = test_predictors)


postResample(ridge_predictions, testData$GROSS_PROFIT_DEAD_NET)

mse <- mean((test_response - ridge_predictions)^2)
print(paste("Test set mean squared error:", mse))
```

**Elastic net regression**
```{r}
# Fit the Elastic net model
#install.packages("glmnet")
library(glmnet)

fit <- glmnet(x = predictors_matrix, y = pred_res, alpha = 0.5, lambda = seq(0.1, 1, by = 0.1))


# Predict the outcome variable on the test set
elas_prediction <- predict(fit, newx = test_predictors, s = 0.5)

# Calculate the mean squared error
postResample(elas_prediction, testData$GROSS_PROFIT_DEAD_NET)
mse <- mean((test_response - ridge_predictions)^2)
print(paste("Test set mean squared error:", mse))

```
Elastic net regression is a regularized linear regression method that combines L1 and L2 regularization. It is used to prevent overfitting and improve the accuracy of the model when there are many features.

The L1 regularization term encourages sparsity in the coefficient estimates, which means that some of the coefficients are set to zero, effectively removing those features from the model. The L2 regularization term, on the other hand, penalizes large coefficients, which helps to prevent overfitting.

The Elastic net method combines these two regularization methods and allows for a flexible trade-off between sparsity and smoothness in the coefficient estimates.




**Gradient Boosting Regression**
```{r}

# Load the gbm package
#install.packages("gbm")
library(gbm)

# Fit a gradient boosting regression model with 100 trees and 0.01 shrinkage

gb_model <- gbm(GROSS_PROFIT_DEAD_NET ~ ., data = trainData[,-c(1,2,13,14,18)], distribution = "gaussian", n.trees = 1000, interaction.depth = 3, shrinkage = 0.01)

```

```{r}
# Make predictions on the test set
gb_pred <- predict(gb_model, newdata = testData, n.trees = 1000)

# Evaluate the performance of the model on the test set
postResample(gb_pred, testData$GROSS_PROFIT_DEAD_NET)
rf_mse <- mean((testData$GROSS_PROFIT_DEAD_NET - gb_pred)^2)
print(paste("Random Forest Regression MSE:", rf_mse))

```


Strengths:

Can handle non-linear relationships between the predictor and outcome variables.
Can handle highly correlated predictor variables.
Can handle missing data.
Can automatically detect important predictor variables.
Can be used for both regression and classification problems.
Tends to produce very accurate predictions when tuned properly.

Weaknesses:

Can be computationally intensive when there are many predictor variables or a large number of trees in the model.
Requires careful tuning of hyperparameters to avoid overfitting or underfitting.
Can be difficult to interpret the model and identify important predictor variables.
May be sensitive to outliers in the data.
Can be prone to overfitting if the number of trees is too high or the learning rate is too low.
Overall, gradient boosting regression is a powerful and flexible machine learning algorithm that can be used to accurately predict continuous outcome variables. However, it does require careful tuning of hyperparameters to ensure that the model is not overfitting or underfitting the data. Additionally, the model can be difficult to interpret and may be sensitive to outliers in the data. Nonetheless, when properly tuned, gradient boosting regression is a popular and effective approach for many predictive modeling tasks.


**Neural Network Regression**


```{r}
# Load the nnet package
#install.packages("nnet")
library(nnet)

# Fit a neural network regression model using only the selected predictors
nn_model <- nnet(GROSS_PROFIT_DEAD_NET ~ ., data = trainData[,c(3,4,5,7,8,9,10,11,15,16,17)], size = 1, linout = TRUE)

# Predict the outcome variable on the test set using the selected predictors
pred_nn <- predict(nn_model, newdata = testData[,c(3,4,5,7,8,9,10,11,15,16,17)])


```

```{r}
# Make predictions on the test set
nn_pred <- predict(nn_model, newdata = testData)

# Evaluate the performance of the model on the test set
postResample(nn_pred, testData$GROSS_PROFIT_DEAD_NET)
nn_mse <- mean((testData$GROSS_PROFIT_DEAD_NET - nn_pred)^2)
print(paste("Neural Network Regression MSE:", nn_mse))
```
Strengths:

Can model non-linear relationships between the predictor and outcome variables.
Can handle highly correlated predictor variables.
Can handle missing data.
Can automatically detect important predictor variables.
Can be used for both regression and classification problems.
Can be used for high-dimensional datasets with many predictor variables.

Weaknesses:
Can be computationally intensive when there are many predictor variables or a large number of hidden layers in the model.
Requires careful tuning of hyperparameters to avoid overfitting or underfitting.
Can be difficult to interpret the model and identify important predictor variables.
May be sensitive to outliers in the data.
Can be prone to overfitting if the model is too complex.



Compare the results and pick the one that has a better prediction
```{r}
postResample(lasso_predictions, testData$GROSS_PROFIT_DEAD_NET)
postResample(ridge_predictions, testData$GROSS_PROFIT_DEAD_NET)
postResample(elas_prediction, testData$GROSS_PROFIT_DEAD_NET)
postResample(gb_pred, testData$GROSS_PROFIT_DEAD_NET)
postResample(nn_pred, testData$GROSS_PROFIT_DEAD_NET)
```

Perform business validation of the model.  Are your results sufficient to solve the business problem?

Based on the results, I would choos Lasso Regression Model, since it has lower MSRE, higher R-Squared. The R-Squared value is  0.925 which indicates that 92.57% of the variance in the dependent variable is explained by the independent variables in the model. 





